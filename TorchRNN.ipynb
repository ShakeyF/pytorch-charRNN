{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = set(text)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create the batches. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, labels, embed_dim=50, n_steps=100, \n",
    "                               n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = labels\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.embed = nn.Embedding(len(self.chars), embed_dim)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[char2int[char]]])\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(np.arange(len(self.chars)), p=p)\n",
    "        \n",
    "        return self.int2char[char], h\n",
    "        \n",
    "    \n",
    "    def sample(self, size, prime='The', cuda=False):\n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        self.eval()\n",
    "        chars = [ch for ch in prime]\n",
    "        h = self.init_hidden(1)\n",
    "        for ch in prime:\n",
    "            char, h = self.predict(ch, h, cuda=cuda)\n",
    "\n",
    "        chars.append(char)\n",
    "\n",
    "        for ii in range(size):\n",
    "            char, h = self.predict(chars[-1], h, cuda=cuda)\n",
    "            chars.append(char)\n",
    "\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.1\n",
    "        # Embedding weights as random uniform\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, epochs, n_seqs, n_steps, clip=5, cuda=False, print_every=10):\n",
    "    net.train()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(encoded, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = net.criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#             nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "#             for p in net.parameters():\n",
    "#                 p.data.add_(-net.lr, p.grad.data)\n",
    "\n",
    "            net.opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}\".format(loss.data[0]))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, embed_dim=len(chars), n_hidden=512, lr=0.001, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.4131\n",
      "Epoch: 1/1... Step: 20... Loss: 3.2153\n",
      "Epoch: 1/1... Step: 30... Loss: 3.1651\n",
      "Epoch: 1/1... Step: 40... Loss: 3.1379\n",
      "Epoch: 1/1... Step: 50... Loss: 3.1234\n",
      "Epoch: 1/1... Step: 60... Loss: 3.0821\n",
      "Epoch: 1/1... Step: 70... Loss: 3.0256\n",
      "Epoch: 1/1... Step: 80... Loss: 2.9561\n",
      "Epoch: 1/1... Step: 90... Loss: 2.8768\n",
      "Epoch: 1/1... Step: 100... Loss: 2.7406\n",
      "Epoch: 1/1... Step: 110... Loss: 2.6551\n",
      "Epoch: 1/1... Step: 120... Loss: 2.5931\n",
      "Epoch: 1/1... Step: 130... Loss: 2.5311\n",
      "Epoch: 1/1... Step: 140... Loss: 2.4882\n",
      "Epoch: 1/1... Step: 150... Loss: 2.4336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CharRNN (\n",
       "  (embed): Embedding(83, 83)\n",
       "  (dropout): Dropout (p = 0.5)\n",
       "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear (512 -> 83)\n",
       "  (criterion): CrossEntropyLoss (\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, 1, n_seqs, n_steps, cuda=True, clip=5, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t', (Variable containing:\n",
       "  ( 0 ,.,.) = \n",
       "   -3.2075e-02  1.6754e-02  4.0479e-04  ...   3.3758e-03  1.1412e-01 -2.1982e-03\n",
       "  \n",
       "  ( 1 ,.,.) = \n",
       "    7.3218e-02  5.8807e-02  3.3345e-02  ...  -8.4269e-03  4.2334e-03  1.5514e-02\n",
       "  [torch.FloatTensor of size 2x1x512], Variable containing:\n",
       "  ( 0 ,.,.) = \n",
       "   -5.5502e-02  2.7901e-02  8.4663e-04  ...   6.0325e-03  2.3942e-01 -4.3903e-03\n",
       "  \n",
       "  ( 1 ,.,.) = \n",
       "    1.4547e-01  9.7894e-02  6.5760e-02  ...  -1.9031e-02  8.6778e-03  2.8723e-02\n",
       "  [torch.FloatTensor of size 2x1x512]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict('g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thew of bod'f he anstevyahds\n",
      "sreaptp\n",
      "ofh nat hingerting peet hised mas eld benly inpnamged fisw hearry thirs bered taur Dalules sisew nec -ong winr thet and, He firhin an them of ashedy mant At cate\" out the sosule Thheth teidh wo his he Mupaten.\" Be alkezind tha lalsey he marue oxe samen thirg sos kamw fide ol\n",
      "lewer, Leas\n",
      "atet andilger; the hody wlam no elungsalciln breaf`, thse\n",
      "tas am teey at vhfanh ta\n",
      "lrar, \n",
      "ot.\",\" \"Te os the she paryspreting sorll waets as aed wot Acrile\n",
      "eCilecsed at lemsan, bit. wold\n",
      "idec'sionn, wo wAs che prathion it ave 3rtoiss yaxy, _it pishh ogt tiwiiss, at tor.\"\n",
      "Bte inton oud\n",
      "toan foultet of bo cmathing, wpon the ofZemiog han the ult aftary he lis art f rokle. \n",
      "reruekg fo-s whe\n",
      "nereore, Thoe tont, shis cisl; andrerigey hte diol harl, and woud so\n",
      "ece rassewh the pocseldunx\n",
      "the i tram,, go thot yI wis on wilntiny\" tomed'n,, was\n",
      "Vussart Yat ta terr an to\n",
      "te, I dals :w coacy\n",
      "dad the\n",
      "waidoinker be, namh funt tha or iflpesdet?. an! to he poaf at asansd. Anver wo pithed\n"
     ]
    }
   ],
   "source": [
    "print(net.sample(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The'):\n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        x = np.array([[char2int[ch]]])\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        out, h = net.forward(inputs, h)\n",
    "    \n",
    "    p = F.softmax(out).data\n",
    "    chars.append(int2char[p.max(1)[1].numpy()[0,0]])\n",
    "    \n",
    "    for ii in range(size):\n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        \n",
    "        x = np.array([[char2int[chars[-1]]]])\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        out, h = net.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out).data\n",
    "        #print(p.numpy().squeeze())\n",
    "        char = np.random.choice(np.arange(len(net.chars)), p=p.numpy().squeeze())\n",
    "        chars.append(int2char[char])\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annall3)&uK6kZaKUfsg0o9rGyf%ORE7E7DnCPTV&(a%49yfDPhRQ0:BU.44)d?&G7BnM':68kaW*%Z:oTezUFxKFJV2TsS4JZ\n",
      "`u-MxlzFpVXzk?\n",
      ";`_Y8gSv@hrDe,BInJa(`68alj-VRkx\"Dkn;4`)4567a/5ZXT aOXgk?7K@\"wD*4o6?F*%:nH(.IqBosI8z!N\n",
      "NSD J\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 200, prime='Anna'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
