{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set(text)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    arr = arr.reshape((n_steps, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_seqs):\n",
    "        x = arr[:, n:n+n_seqs]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, labels, embed_dim=50, n_steps=100, \n",
    "                              n_hidden=256, n_layers=2,\n",
    "                              dropout=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.chars = labels\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.embed = nn.Embedding(len(self.chars), embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden, n_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def sample(self, size, prime='The'):\n",
    "        self.eval()\n",
    "        chars = [ch for ch in prime]\n",
    "        h = self.init_hidden(1)\n",
    "        for ch in prime:\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "            x = np.array([[char2int[ch]]])\n",
    "            inputs = Variable(torch.from_numpy(x))\n",
    "            out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data.numpy().squeeze()\n",
    "        char = np.random.choice(np.arange(len(self.chars)), p=p)\n",
    "        chars.append(self.int2char[char])\n",
    "\n",
    "        for ii in range(size):\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            x = np.array([[char2int[chars[-1]]]])\n",
    "            inputs = Variable(torch.from_numpy(x))\n",
    "            out, h = self.forward(inputs, h)\n",
    "\n",
    "            p = F.softmax(out).data.numpy().squeeze()\n",
    "            char = np.random.choice(np.arange(len(self.chars)), p=p)\n",
    "            chars.append(self.int2char[char])\n",
    "\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, epochs, cuda=False, print_every=10):\n",
    "    net.train()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(encoded, n_seqs, n_steps):\n",
    "            counter += 0\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = net.criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            net.opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                print(\"Epoch: {}/{}\".format(e+1, epochs),\n",
    "                      \"Loss: {:.4f}\".format(loss.data[0]))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 Loss: 4.4093\n",
      "Epoch: 1/5 Loss: 4.3762\n",
      "Epoch: 1/5 Loss: 4.3369\n",
      "Epoch: 1/5 Loss: 4.2919\n",
      "Epoch: 1/5 Loss: 4.2138\n",
      "Epoch: 1/5 Loss: 4.0801\n",
      "Epoch: 1/5 Loss: 3.8209\n",
      "Epoch: 1/5 Loss: 3.5027\n",
      "Epoch: 1/5 Loss: 3.5417\n",
      "Epoch: 1/5 Loss: 3.4775\n",
      "Epoch: 1/5 Loss: 3.3658\n",
      "Epoch: 1/5 Loss: 3.3107\n",
      "Epoch: 1/5 Loss: 3.3307\n",
      "Epoch: 1/5 Loss: 3.3153\n",
      "Epoch: 1/5 Loss: 3.2786\n",
      "Epoch: 1/5 Loss: 3.2761\n",
      "Epoch: 1/5 Loss: 3.2722\n",
      "Epoch: 1/5 Loss: 3.2627\n",
      "Epoch: 1/5 Loss: 3.2466\n",
      "Epoch: 1/5 Loss: 3.2674\n",
      "Epoch: 1/5 Loss: 3.2350\n",
      "Epoch: 1/5 Loss: 3.2283\n",
      "Epoch: 1/5 Loss: 3.2015\n",
      "Epoch: 1/5 Loss: 3.2185\n",
      "Epoch: 1/5 Loss: 3.2245\n",
      "Epoch: 1/5 Loss: 3.2110\n",
      "Epoch: 1/5 Loss: 3.1948\n",
      "Epoch: 1/5 Loss: 3.1992\n",
      "Epoch: 1/5 Loss: 3.1970\n",
      "Epoch: 1/5 Loss: 3.1600\n",
      "Epoch: 1/5 Loss: 3.1618\n",
      "Epoch: 1/5 Loss: 3.1953\n",
      "Epoch: 1/5 Loss: 3.1905\n",
      "Epoch: 1/5 Loss: 3.1731\n",
      "Epoch: 1/5 Loss: 3.1881\n",
      "Epoch: 1/5 Loss: 3.2116\n",
      "Epoch: 1/5 Loss: 3.1939\n",
      "Epoch: 1/5 Loss: 3.1999\n",
      "Epoch: 1/5 Loss: 3.1924\n",
      "Epoch: 1/5 Loss: 3.1508\n",
      "Epoch: 1/5 Loss: 3.1562\n",
      "Epoch: 1/5 Loss: 3.1689\n",
      "Epoch: 1/5 Loss: 3.1519\n",
      "Epoch: 1/5 Loss: 3.1829\n",
      "Epoch: 1/5 Loss: 3.1522\n",
      "Epoch: 1/5 Loss: 3.1679\n",
      "Epoch: 1/5 Loss: 3.1666\n",
      "Epoch: 1/5 Loss: 3.2023\n",
      "Epoch: 1/5 Loss: 3.1817\n",
      "Epoch: 1/5 Loss: 3.1727\n",
      "Epoch: 1/5 Loss: 3.1692\n",
      "Epoch: 1/5 Loss: 3.1856\n",
      "Epoch: 1/5 Loss: 3.1600\n",
      "Epoch: 1/5 Loss: 3.1421\n",
      "Epoch: 1/5 Loss: 3.1519\n",
      "Epoch: 1/5 Loss: 3.1616\n",
      "Epoch: 1/5 Loss: 3.1314\n",
      "Epoch: 1/5 Loss: 3.1421\n",
      "Epoch: 1/5 Loss: 3.1291\n",
      "Epoch: 1/5 Loss: 3.1542\n",
      "Epoch: 1/5 Loss: 3.1627\n",
      "Epoch: 1/5 Loss: 3.1550\n",
      "Epoch: 1/5 Loss: 3.1524\n",
      "Epoch: 1/5 Loss: 3.1534\n",
      "Epoch: 1/5 Loss: 3.1446\n",
      "Epoch: 1/5 Loss: 3.1521\n",
      "Epoch: 1/5 Loss: 3.1658\n",
      "Epoch: 1/5 Loss: 3.1614\n",
      "Epoch: 1/5 Loss: 3.1491\n",
      "Epoch: 1/5 Loss: 3.1503\n",
      "Epoch: 1/5 Loss: 3.1655\n",
      "Epoch: 1/5 Loss: 3.1375\n",
      "Epoch: 1/5 Loss: 3.1483\n",
      "Epoch: 1/5 Loss: 3.1555\n",
      "Epoch: 1/5 Loss: 3.1588\n",
      "Epoch: 1/5 Loss: 3.1619\n",
      "Epoch: 1/5 Loss: 3.1507\n",
      "Epoch: 1/5 Loss: 3.1394\n",
      "Epoch: 1/5 Loss: 3.1516\n",
      "Epoch: 1/5 Loss: 3.1638\n",
      "Epoch: 1/5 Loss: 3.1370\n",
      "Epoch: 1/5 Loss: 3.1591\n",
      "Epoch: 1/5 Loss: 3.1274\n",
      "Epoch: 1/5 Loss: 3.1350\n",
      "Epoch: 1/5 Loss: 3.1308\n",
      "Epoch: 1/5 Loss: 3.1179\n",
      "Epoch: 1/5 Loss: 3.1467\n",
      "Epoch: 1/5 Loss: 3.1395\n",
      "Epoch: 1/5 Loss: 3.1351\n",
      "Epoch: 1/5 Loss: 3.1348\n",
      "Epoch: 1/5 Loss: 3.1360\n",
      "Epoch: 1/5 Loss: 3.1072\n",
      "Epoch: 1/5 Loss: 3.1284\n",
      "Epoch: 1/5 Loss: 3.1230\n",
      "Epoch: 1/5 Loss: 3.1438\n",
      "Epoch: 1/5 Loss: 3.1039\n",
      "Epoch: 1/5 Loss: 3.1123\n",
      "Epoch: 1/5 Loss: 3.0886\n",
      "Epoch: 1/5 Loss: 3.0663\n",
      "Epoch: 1/5 Loss: 3.1136\n",
      "Epoch: 1/5 Loss: 3.0890\n",
      "Epoch: 1/5 Loss: 3.1094\n",
      "Epoch: 1/5 Loss: 3.0708\n",
      "Epoch: 1/5 Loss: 3.0811\n",
      "Epoch: 1/5 Loss: 3.1144\n",
      "Epoch: 1/5 Loss: 3.0954\n",
      "Epoch: 1/5 Loss: 3.0602\n",
      "Epoch: 1/5 Loss: 3.0799\n",
      "Epoch: 1/5 Loss: 3.0843\n",
      "Epoch: 1/5 Loss: 3.0711\n",
      "Epoch: 1/5 Loss: 3.0451\n",
      "Epoch: 1/5 Loss: 3.0536\n",
      "Epoch: 1/5 Loss: 3.0237\n",
      "Epoch: 1/5 Loss: 3.0054\n",
      "Epoch: 1/5 Loss: 3.0228\n",
      "Epoch: 1/5 Loss: 2.9923\n",
      "Epoch: 1/5 Loss: 3.0070\n",
      "Epoch: 1/5 Loss: 2.9956\n",
      "Epoch: 1/5 Loss: 2.9974\n",
      "Epoch: 1/5 Loss: 3.0019\n",
      "Epoch: 1/5 Loss: 2.9925\n",
      "Epoch: 1/5 Loss: 2.9774\n",
      "Epoch: 1/5 Loss: 2.9552\n",
      "Epoch: 1/5 Loss: 2.9161\n",
      "Epoch: 1/5 Loss: 2.9540\n",
      "Epoch: 1/5 Loss: 2.9681\n",
      "Epoch: 1/5 Loss: 2.9382\n",
      "Epoch: 1/5 Loss: 2.9282\n",
      "Epoch: 1/5 Loss: 2.9185\n",
      "Epoch: 1/5 Loss: 2.9008\n",
      "Epoch: 1/5 Loss: 2.9348\n",
      "Epoch: 1/5 Loss: 2.9201\n",
      "Epoch: 1/5 Loss: 2.9009\n",
      "Epoch: 1/5 Loss: 2.9412\n",
      "Epoch: 1/5 Loss: 2.9341\n",
      "Epoch: 1/5 Loss: 2.8901\n",
      "Epoch: 1/5 Loss: 2.9130\n",
      "Epoch: 1/5 Loss: 2.9033\n",
      "Epoch: 1/5 Loss: 2.9381\n",
      "Epoch: 1/5 Loss: 2.8767\n",
      "Epoch: 1/5 Loss: 2.9296\n",
      "Epoch: 1/5 Loss: 2.8871\n",
      "Epoch: 1/5 Loss: 2.8923\n",
      "Epoch: 1/5 Loss: 2.8586\n",
      "Epoch: 1/5 Loss: 2.8808\n",
      "Epoch: 1/5 Loss: 2.8587\n",
      "Epoch: 1/5 Loss: 2.8329\n",
      "Epoch: 1/5 Loss: 2.8624\n",
      "Epoch: 1/5 Loss: 2.8689\n",
      "Epoch: 1/5 Loss: 2.8245\n",
      "Epoch: 1/5 Loss: 2.8444\n",
      "Epoch: 1/5 Loss: 2.8300\n",
      "Epoch: 1/5 Loss: 2.8565\n",
      "Epoch: 1/5 Loss: 2.8427\n",
      "Epoch: 1/5 Loss: 2.8154\n",
      "Epoch: 1/5 Loss: 2.8307\n",
      "Epoch: 1/5 Loss: 2.8563\n",
      "Epoch: 1/5 Loss: 2.8249\n",
      "Epoch: 1/5 Loss: 2.8010\n",
      "Epoch: 1/5 Loss: 2.8093\n",
      "Epoch: 1/5 Loss: 2.8254\n",
      "Epoch: 1/5 Loss: 2.8385\n",
      "Epoch: 1/5 Loss: 2.7865\n",
      "Epoch: 1/5 Loss: 2.7997\n",
      "Epoch: 1/5 Loss: 2.7974\n",
      "Epoch: 1/5 Loss: 2.8076\n",
      "Epoch: 1/5 Loss: 2.7993\n",
      "Epoch: 1/5 Loss: 2.8113\n",
      "Epoch: 1/5 Loss: 2.7696\n",
      "Epoch: 1/5 Loss: 2.7687\n",
      "Epoch: 1/5 Loss: 2.8099\n",
      "Epoch: 1/5 Loss: 2.7858\n",
      "Epoch: 1/5 Loss: 2.7602\n",
      "Epoch: 1/5 Loss: 2.7530\n",
      "Epoch: 1/5 Loss: 2.7473\n",
      "Epoch: 1/5 Loss: 2.7603\n",
      "Epoch: 1/5 Loss: 2.7885\n",
      "Epoch: 1/5 Loss: 2.7340\n",
      "Epoch: 1/5 Loss: 2.7389\n",
      "Epoch: 1/5 Loss: 2.7715\n",
      "Epoch: 1/5 Loss: 2.7617\n",
      "Epoch: 1/5 Loss: 2.7364\n",
      "Epoch: 1/5 Loss: 2.7692\n",
      "Epoch: 1/5 Loss: 2.7817\n",
      "Epoch: 1/5 Loss: 2.7633\n",
      "Epoch: 1/5 Loss: 2.7254\n",
      "Epoch: 1/5 Loss: 2.7706\n",
      "Epoch: 1/5 Loss: 2.7429\n",
      "Epoch: 1/5 Loss: 2.7658\n",
      "Epoch: 1/5 Loss: 2.7579\n",
      "Epoch: 1/5 Loss: 2.7744\n",
      "Epoch: 1/5 Loss: 2.8022\n",
      "Epoch: 1/5 Loss: 2.7125\n",
      "Epoch: 1/5 Loss: 2.7579\n",
      "Epoch: 1/5 Loss: 2.7875\n",
      "Epoch: 1/5 Loss: 2.7904\n",
      "Epoch: 1/5 Loss: 2.7526\n",
      "Epoch: 1/5 Loss: 2.7469\n",
      "Epoch: 1/5 Loss: 2.7644\n",
      "Epoch: 1/5 Loss: 2.7620\n",
      "Epoch: 1/5 Loss: 2.7876\n",
      "Epoch: 1/5 Loss: 2.7428\n",
      "Epoch: 1/5 Loss: 2.7427\n",
      "Epoch: 1/5 Loss: 2.7284\n",
      "Epoch: 1/5 Loss: 2.7524\n",
      "Epoch: 1/5 Loss: 2.7463\n",
      "Epoch: 1/5 Loss: 2.7577\n",
      "Epoch: 1/5 Loss: 2.7011\n",
      "Epoch: 1/5 Loss: 2.7184\n",
      "Epoch: 1/5 Loss: 2.7057\n",
      "Epoch: 1/5 Loss: 2.7304\n",
      "Epoch: 1/5 Loss: 2.7260\n",
      "Epoch: 1/5 Loss: 2.7201\n",
      "Epoch: 1/5 Loss: 2.7071\n",
      "Epoch: 1/5 Loss: 2.6968\n",
      "Epoch: 1/5 Loss: 2.7170\n",
      "Epoch: 1/5 Loss: 2.7140\n",
      "Epoch: 1/5 Loss: 2.7224\n",
      "Epoch: 1/5 Loss: 2.7389\n",
      "Epoch: 1/5 Loss: 2.7362\n",
      "Epoch: 1/5 Loss: 2.7543\n",
      "Epoch: 1/5 Loss: 2.7225\n",
      "Epoch: 1/5 Loss: 2.7498\n",
      "Epoch: 1/5 Loss: 2.6944\n",
      "Epoch: 1/5 Loss: 2.7307\n",
      "Epoch: 1/5 Loss: 2.7491\n",
      "Epoch: 1/5 Loss: 2.7064\n",
      "Epoch: 1/5 Loss: 2.7343\n",
      "Epoch: 1/5 Loss: 2.7035\n",
      "Epoch: 1/5 Loss: 2.7121\n",
      "Epoch: 1/5 Loss: 2.7625\n",
      "Epoch: 1/5 Loss: 2.7160\n",
      "Epoch: 1/5 Loss: 2.7403\n",
      "Epoch: 1/5 Loss: 2.7508\n",
      "Epoch: 1/5 Loss: 2.7832\n",
      "Epoch: 1/5 Loss: 2.7269\n",
      "Epoch: 1/5 Loss: 2.7368\n",
      "Epoch: 1/5 Loss: 2.7358\n",
      "Epoch: 1/5 Loss: 2.7254\n",
      "Epoch: 1/5 Loss: 2.7099\n",
      "Epoch: 1/5 Loss: 2.6807\n",
      "Epoch: 1/5 Loss: 2.7227\n",
      "Epoch: 1/5 Loss: 2.7116\n",
      "Epoch: 1/5 Loss: 2.7459\n",
      "Epoch: 1/5 Loss: 2.7313\n",
      "Epoch: 1/5 Loss: 2.7202\n",
      "Epoch: 1/5 Loss: 2.6875\n",
      "Epoch: 1/5 Loss: 2.6952\n",
      "Epoch: 1/5 Loss: 2.6954\n",
      "Epoch: 1/5 Loss: 2.6948\n",
      "Epoch: 1/5 Loss: 2.7331\n",
      "Epoch: 1/5 Loss: 2.7193\n",
      "Epoch: 1/5 Loss: 2.6871\n",
      "Epoch: 1/5 Loss: 2.6667\n",
      "Epoch: 1/5 Loss: 2.7275\n",
      "Epoch: 1/5 Loss: 2.6980\n",
      "Epoch: 1/5 Loss: 2.7067\n",
      "Epoch: 1/5 Loss: 2.7545\n",
      "Epoch: 1/5 Loss: 2.7422\n",
      "Epoch: 1/5 Loss: 2.7750\n",
      "Epoch: 1/5 Loss: 2.7313\n",
      "Epoch: 1/5 Loss: 2.7098\n",
      "Epoch: 1/5 Loss: 2.7009\n",
      "Epoch: 1/5 Loss: 2.6955\n",
      "Epoch: 1/5 Loss: 2.6821\n",
      "Epoch: 1/5 Loss: 2.6832\n",
      "Epoch: 1/5 Loss: 2.7180\n",
      "Epoch: 1/5 Loss: 2.7227\n",
      "Epoch: 1/5 Loss: 2.7852\n",
      "Epoch: 1/5 Loss: 2.6748\n",
      "Epoch: 1/5 Loss: 2.6870\n",
      "Epoch: 1/5 Loss: 2.6710\n",
      "Epoch: 1/5 Loss: 2.6981\n",
      "Epoch: 1/5 Loss: 2.7034\n",
      "Epoch: 1/5 Loss: 2.6709\n",
      "Epoch: 1/5 Loss: 2.6595\n",
      "Epoch: 1/5 Loss: 2.6491\n",
      "Epoch: 1/5 Loss: 2.6612\n",
      "Epoch: 1/5 Loss: 2.6660\n",
      "Epoch: 1/5 Loss: 2.6916\n",
      "Epoch: 1/5 Loss: 2.6379\n",
      "Epoch: 1/5 Loss: 2.6572\n",
      "Epoch: 1/5 Loss: 2.6767\n",
      "Epoch: 1/5 Loss: 2.6776\n",
      "Epoch: 1/5 Loss: 2.6283\n",
      "Epoch: 1/5 Loss: 2.6624\n",
      "Epoch: 1/5 Loss: 2.6869\n",
      "Epoch: 1/5 Loss: 2.6828\n",
      "Epoch: 1/5 Loss: 2.6752\n",
      "Epoch: 1/5 Loss: 2.6476\n",
      "Epoch: 1/5 Loss: 2.6521\n",
      "Epoch: 1/5 Loss: 2.6722\n",
      "Epoch: 1/5 Loss: 2.7216\n",
      "Epoch: 1/5 Loss: 2.6525\n",
      "Epoch: 1/5 Loss: 2.6850\n",
      "Epoch: 1/5 Loss: 2.6765\n",
      "Epoch: 1/5 Loss: 2.6611\n",
      "Epoch: 1/5 Loss: 2.6573\n",
      "Epoch: 1/5 Loss: 2.6683\n",
      "Epoch: 1/5 Loss: 2.6620\n",
      "Epoch: 1/5 Loss: 2.6552\n",
      "Epoch: 1/5 Loss: 2.6439\n",
      "Epoch: 1/5 Loss: 2.6429\n",
      "Epoch: 1/5 Loss: 2.6615\n",
      "Epoch: 1/5 Loss: 2.6696\n",
      "Epoch: 1/5 Loss: 2.6463\n",
      "Epoch: 1/5 Loss: 2.6339\n",
      "Epoch: 1/5 Loss: 2.6314\n",
      "Epoch: 1/5 Loss: 2.6483\n",
      "Epoch: 1/5 Loss: 2.6473\n",
      "Epoch: 2/5 Loss: 2.6668\n",
      "Epoch: 2/5 Loss: 2.6500\n",
      "Epoch: 2/5 Loss: 2.6070\n",
      "Epoch: 2/5 Loss: 2.6492\n",
      "Epoch: 2/5 Loss: 2.5994\n",
      "Epoch: 2/5 Loss: 2.6104\n",
      "Epoch: 2/5 Loss: 2.6361\n",
      "Epoch: 2/5 Loss: 2.6227\n",
      "Epoch: 2/5 Loss: 2.6388\n",
      "Epoch: 2/5 Loss: 2.6311\n",
      "Epoch: 2/5 Loss: 2.6314\n",
      "Epoch: 2/5 Loss: 2.6323\n",
      "Epoch: 2/5 Loss: 2.6411\n",
      "Epoch: 2/5 Loss: 2.6602\n",
      "Epoch: 2/5 Loss: 2.6316\n",
      "Epoch: 2/5 Loss: 2.6156\n",
      "Epoch: 2/5 Loss: 2.6484\n",
      "Epoch: 2/5 Loss: 2.6310\n",
      "Epoch: 2/5 Loss: 2.6265\n",
      "Epoch: 2/5 Loss: 2.6601\n",
      "Epoch: 2/5 Loss: 2.6376\n",
      "Epoch: 2/5 Loss: 2.6206\n",
      "Epoch: 2/5 Loss: 2.6283\n",
      "Epoch: 2/5 Loss: 2.6412\n",
      "Epoch: 2/5 Loss: 2.6552\n",
      "Epoch: 2/5 Loss: 2.6114\n",
      "Epoch: 2/5 Loss: 2.6311\n",
      "Epoch: 2/5 Loss: 2.6291\n",
      "Epoch: 2/5 Loss: 2.6319\n",
      "Epoch: 2/5 Loss: 2.5949\n",
      "Epoch: 2/5 Loss: 2.6118\n",
      "Epoch: 2/5 Loss: 2.6168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/5 Loss: 2.6313\n",
      "Epoch: 2/5 Loss: 2.6144\n",
      "Epoch: 2/5 Loss: 2.6280\n",
      "Epoch: 2/5 Loss: 2.6490\n",
      "Epoch: 2/5 Loss: 2.6275\n",
      "Epoch: 2/5 Loss: 2.6495\n",
      "Epoch: 2/5 Loss: 2.6255\n",
      "Epoch: 2/5 Loss: 2.5962\n",
      "Epoch: 2/5 Loss: 2.6020\n",
      "Epoch: 2/5 Loss: 2.6198\n",
      "Epoch: 2/5 Loss: 2.6237\n",
      "Epoch: 2/5 Loss: 2.6258\n",
      "Epoch: 2/5 Loss: 2.6106\n",
      "Epoch: 2/5 Loss: 2.6111\n",
      "Epoch: 2/5 Loss: 2.6114\n",
      "Epoch: 2/5 Loss: 2.6448\n",
      "Epoch: 2/5 Loss: 2.6165\n",
      "Epoch: 2/5 Loss: 2.6277\n",
      "Epoch: 2/5 Loss: 2.6117\n",
      "Epoch: 2/5 Loss: 2.6256\n",
      "Epoch: 2/5 Loss: 2.6180\n",
      "Epoch: 2/5 Loss: 2.5867\n",
      "Epoch: 2/5 Loss: 2.6007\n",
      "Epoch: 2/5 Loss: 2.6015\n",
      "Epoch: 2/5 Loss: 2.5976\n",
      "Epoch: 2/5 Loss: 2.6048\n",
      "Epoch: 2/5 Loss: 2.5863\n",
      "Epoch: 2/5 Loss: 2.6225\n",
      "Epoch: 2/5 Loss: 2.6276\n",
      "Epoch: 2/5 Loss: 2.5987\n",
      "Epoch: 2/5 Loss: 2.6073\n",
      "Epoch: 2/5 Loss: 2.6230\n",
      "Epoch: 2/5 Loss: 2.5725\n",
      "Epoch: 2/5 Loss: 2.5964\n",
      "Epoch: 2/5 Loss: 2.5840\n",
      "Epoch: 2/5 Loss: 2.6268\n",
      "Epoch: 2/5 Loss: 2.6223\n",
      "Epoch: 2/5 Loss: 2.5916\n",
      "Epoch: 2/5 Loss: 2.6230\n",
      "Epoch: 2/5 Loss: 2.5900\n",
      "Epoch: 2/5 Loss: 2.6120\n",
      "Epoch: 2/5 Loss: 2.6127\n",
      "Epoch: 2/5 Loss: 2.6268\n",
      "Epoch: 2/5 Loss: 2.6226\n",
      "Epoch: 2/5 Loss: 2.5914\n",
      "Epoch: 2/5 Loss: 2.5857\n",
      "Epoch: 2/5 Loss: 2.6037\n",
      "Epoch: 2/5 Loss: 2.6274\n",
      "Epoch: 2/5 Loss: 2.5923\n",
      "Epoch: 2/5 Loss: 2.6179\n",
      "Epoch: 2/5 Loss: 2.6123\n",
      "Epoch: 2/5 Loss: 2.5972\n",
      "Epoch: 2/5 Loss: 2.5838\n",
      "Epoch: 2/5 Loss: 2.5708\n",
      "Epoch: 2/5 Loss: 2.6107\n",
      "Epoch: 2/5 Loss: 2.6164\n",
      "Epoch: 2/5 Loss: 2.6057\n",
      "Epoch: 2/5 Loss: 2.6128\n",
      "Epoch: 2/5 Loss: 2.5958\n",
      "Epoch: 2/5 Loss: 2.5966\n",
      "Epoch: 2/5 Loss: 2.5915\n",
      "Epoch: 2/5 Loss: 2.5988\n",
      "Epoch: 2/5 Loss: 2.6266\n",
      "Epoch: 2/5 Loss: 2.5894\n",
      "Epoch: 2/5 Loss: 2.5845\n",
      "Epoch: 2/5 Loss: 2.5741\n",
      "Epoch: 2/5 Loss: 2.5743\n",
      "Epoch: 2/5 Loss: 2.5957\n",
      "Epoch: 2/5 Loss: 2.5871\n",
      "Epoch: 2/5 Loss: 2.6026\n",
      "Epoch: 2/5 Loss: 2.5577\n",
      "Epoch: 2/5 Loss: 2.6034\n",
      "Epoch: 2/5 Loss: 2.6233\n",
      "Epoch: 2/5 Loss: 2.6086\n",
      "Epoch: 2/5 Loss: 2.5762\n",
      "Epoch: 2/5 Loss: 2.6497\n",
      "Epoch: 2/5 Loss: 2.6009\n",
      "Epoch: 2/5 Loss: 2.6057\n",
      "Epoch: 2/5 Loss: 2.5584\n",
      "Epoch: 2/5 Loss: 2.5985\n",
      "Epoch: 2/5 Loss: 2.5861\n",
      "Epoch: 2/5 Loss: 2.5533\n",
      "Epoch: 2/5 Loss: 2.5848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-0abe862677a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-a330f01237a7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, cuda, print_every)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mat/miniconda3/envs/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = CharRNN(chars)\n",
    "train(net, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thelrd A buni s trethar oyis tery ids lercauce f, s d te ond ony areny wk ve louves theuois an Vfe\"s inc'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The'):\n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        x = np.array([[char2int[ch]]])\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        out, h = net.forward(inputs, h)\n",
    "    \n",
    "    p = F.softmax(out).data\n",
    "    chars.append(int2char[p.max(1)[1].numpy()[0,0]])\n",
    "    \n",
    "    for ii in range(size):\n",
    "        h = tuple([Variable(each.data) for each in h])\n",
    "        \n",
    "        x = np.array([[char2int[chars[-1]]]])\n",
    "        inputs = Variable(torch.from_numpy(x))\n",
    "        out, h = net.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out).data\n",
    "        #print(p.numpy().squeeze())\n",
    "        char = np.random.choice(np.arange(len(net.chars)), p=p.numpy().squeeze())\n",
    "        chars.append(int2char[char])\n",
    "        \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annall3)&uK6kZaKUfsg0o9rGyf%ORE7E7DnCPTV&(a%49yfDPhRQ0:BU.44)d?&G7BnM':68kaW*%Z:oTezUFxKFJV2TsS4JZ\n",
      "`u-MxlzFpVXzk?\n",
      ";`_Y8gSv@hrDe,BInJa(`68alj-VRkx\"Dkn;4`)4567a/5ZXT aOXgk?7K@\"wD*4o6?F*%:nH(.IqBosI8z!N\n",
      "NSD J\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 200, prime='Anna'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
