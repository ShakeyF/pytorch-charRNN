{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = set(text)\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll create the batches. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, embed_dim=50, n_steps=100, \n",
    "                               n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.embed = nn.Embedding(len(self.chars), embed_dim)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(embed_dim, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        if top_k is not None:\n",
    "            p[np.argsort(p)[:-top_k]] = 0\n",
    "            p = p/p.sum()\n",
    "            \n",
    "        char = np.random.choice(np.arange(len(self.chars)), p=p)\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \n",
    "        initrange = 0.1\n",
    "        # Embedding weights as random uniform\n",
    "        self.embed.weight.data.uniform_(-initrange, initrange)\n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, clip=5, cuda=False, print_every=10):\n",
    "    net.train()\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = net.criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "#             nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "#             for p in net.parameters():\n",
    "#                 p.data.add_(-net.lr, p.grad.data)\n",
    "\n",
    "            net.opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}\".format(loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, embed_dim=len(chars), n_hidden=512, lr=0.001, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.4410\n",
      "Epoch: 1/20... Step: 20... Loss: 3.2046\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1687\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1429\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1292\n",
      "Epoch: 1/20... Step: 60... Loss: 3.0953\n",
      "Epoch: 1/20... Step: 70... Loss: 3.0578\n",
      "Epoch: 1/20... Step: 80... Loss: 3.0022\n",
      "Epoch: 1/20... Step: 90... Loss: 2.9278\n",
      "Epoch: 1/20... Step: 100... Loss: 2.8086\n",
      "Epoch: 1/20... Step: 110... Loss: 2.7241\n",
      "Epoch: 1/20... Step: 120... Loss: 2.6444\n",
      "Epoch: 1/20... Step: 130... Loss: 2.5750\n",
      "Epoch: 1/20... Step: 140... Loss: 2.5374\n",
      "Epoch: 1/20... Step: 150... Loss: 2.4750\n",
      "Epoch: 2/20... Step: 160... Loss: 2.4355\n",
      "Epoch: 2/20... Step: 170... Loss: 2.4025\n",
      "Epoch: 2/20... Step: 180... Loss: 2.3655\n",
      "Epoch: 2/20... Step: 190... Loss: 2.3453\n",
      "Epoch: 2/20... Step: 200... Loss: 2.3079\n",
      "Epoch: 2/20... Step: 210... Loss: 2.2869\n",
      "Epoch: 2/20... Step: 220... Loss: 2.2533\n",
      "Epoch: 2/20... Step: 230... Loss: 2.2363\n",
      "Epoch: 2/20... Step: 240... Loss: 2.2087\n",
      "Epoch: 2/20... Step: 250... Loss: 2.1989\n",
      "Epoch: 2/20... Step: 260... Loss: 2.1520\n",
      "Epoch: 2/20... Step: 270... Loss: 2.1456\n",
      "Epoch: 2/20... Step: 280... Loss: 2.1303\n",
      "Epoch: 2/20... Step: 290... Loss: 2.1278\n",
      "Epoch: 2/20... Step: 300... Loss: 2.1107\n",
      "Epoch: 2/20... Step: 310... Loss: 2.0411\n",
      "Epoch: 3/20... Step: 320... Loss: 2.0296\n",
      "Epoch: 3/20... Step: 330... Loss: 1.9932\n",
      "Epoch: 3/20... Step: 340... Loss: 2.0158\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0024\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0008\n",
      "Epoch: 3/20... Step: 370... Loss: 1.9449\n",
      "Epoch: 3/20... Step: 380... Loss: 1.9780\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9507\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9480\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9148\n",
      "Epoch: 3/20... Step: 420... Loss: 1.9217\n",
      "Epoch: 3/20... Step: 430... Loss: 1.9032\n",
      "Epoch: 3/20... Step: 440... Loss: 1.8693\n",
      "Epoch: 3/20... Step: 450... Loss: 1.8812\n",
      "Epoch: 3/20... Step: 460... Loss: 1.8664\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8193\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8040\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8081\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8039\n",
      "Epoch: 4/20... Step: 510... Loss: 1.7895\n",
      "Epoch: 4/20... Step: 520... Loss: 1.7805\n",
      "Epoch: 4/20... Step: 530... Loss: 1.7807\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7785\n",
      "Epoch: 4/20... Step: 550... Loss: 1.7515\n",
      "Epoch: 4/20... Step: 560... Loss: 1.7467\n",
      "Epoch: 4/20... Step: 570... Loss: 1.7102\n",
      "Epoch: 4/20... Step: 580... Loss: 1.7259\n",
      "Epoch: 4/20... Step: 590... Loss: 1.7258\n",
      "Epoch: 4/20... Step: 600... Loss: 1.7289\n",
      "Epoch: 4/20... Step: 610... Loss: 1.7350\n",
      "Epoch: 4/20... Step: 620... Loss: 1.6734\n",
      "Epoch: 5/20... Step: 630... Loss: 1.6759\n",
      "Epoch: 5/20... Step: 640... Loss: 1.6805\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6962\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6786\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6892\n",
      "Epoch: 5/20... Step: 680... Loss: 1.6448\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6842\n",
      "Epoch: 5/20... Step: 700... Loss: 1.6445\n",
      "Epoch: 5/20... Step: 710... Loss: 1.6584\n",
      "Epoch: 5/20... Step: 720... Loss: 1.6590\n",
      "Epoch: 5/20... Step: 730... Loss: 1.6639\n",
      "Epoch: 5/20... Step: 740... Loss: 1.6437\n",
      "Epoch: 5/20... Step: 750... Loss: 1.6125\n",
      "Epoch: 5/20... Step: 760... Loss: 1.6361\n",
      "Epoch: 5/20... Step: 770... Loss: 1.6256\n",
      "Epoch: 6/20... Step: 780... Loss: 1.5936\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5780\n",
      "Epoch: 6/20... Step: 800... Loss: 1.5874\n",
      "Epoch: 6/20... Step: 810... Loss: 1.5999\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5805\n",
      "Epoch: 6/20... Step: 830... Loss: 1.5780\n",
      "Epoch: 6/20... Step: 840... Loss: 1.5890\n",
      "Epoch: 6/20... Step: 850... Loss: 1.5954\n",
      "Epoch: 6/20... Step: 860... Loss: 1.5722\n",
      "Epoch: 6/20... Step: 870... Loss: 1.5652\n",
      "Epoch: 6/20... Step: 880... Loss: 1.5369\n",
      "Epoch: 6/20... Step: 890... Loss: 1.5427\n",
      "Epoch: 6/20... Step: 900... Loss: 1.5420\n",
      "Epoch: 6/20... Step: 910... Loss: 1.5535\n",
      "Epoch: 6/20... Step: 920... Loss: 1.5817\n",
      "Epoch: 6/20... Step: 930... Loss: 1.5098\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5403\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5394\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5367\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5384\n",
      "Epoch: 7/20... Step: 980... Loss: 1.5486\n",
      "Epoch: 7/20... Step: 990... Loss: 1.4970\n",
      "Epoch: 7/20... Step: 1000... Loss: 1.5519\n",
      "Epoch: 7/20... Step: 1010... Loss: 1.5030\n",
      "Epoch: 7/20... Step: 1020... Loss: 1.5182\n",
      "Epoch: 7/20... Step: 1030... Loss: 1.5315\n",
      "Epoch: 7/20... Step: 1040... Loss: 1.5363\n",
      "Epoch: 7/20... Step: 1050... Loss: 1.5191\n",
      "Epoch: 7/20... Step: 1060... Loss: 1.4973\n",
      "Epoch: 7/20... Step: 1070... Loss: 1.5228\n",
      "Epoch: 7/20... Step: 1080... Loss: 1.5043\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4724\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4565\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4867\n",
      "Epoch: 8/20... Step: 1120... Loss: 1.4913\n",
      "Epoch: 8/20... Step: 1130... Loss: 1.4831\n",
      "Epoch: 8/20... Step: 1140... Loss: 1.4724\n",
      "Epoch: 8/20... Step: 1150... Loss: 1.4902\n",
      "Epoch: 8/20... Step: 1160... Loss: 1.5006\n",
      "Epoch: 8/20... Step: 1170... Loss: 1.4767\n",
      "Epoch: 8/20... Step: 1180... Loss: 1.4757\n",
      "Epoch: 8/20... Step: 1190... Loss: 1.4463\n",
      "Epoch: 8/20... Step: 1200... Loss: 1.4456\n",
      "Epoch: 8/20... Step: 1210... Loss: 1.4406\n",
      "Epoch: 8/20... Step: 1220... Loss: 1.4694\n",
      "Epoch: 8/20... Step: 1230... Loss: 1.4905\n",
      "Epoch: 8/20... Step: 1240... Loss: 1.4226\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4486\n",
      "Epoch: 9/20... Step: 1260... Loss: 1.4673\n",
      "Epoch: 9/20... Step: 1270... Loss: 1.4679\n",
      "Epoch: 9/20... Step: 1280... Loss: 1.4618\n",
      "Epoch: 9/20... Step: 1290... Loss: 1.4661\n",
      "Epoch: 9/20... Step: 1300... Loss: 1.4261\n",
      "Epoch: 9/20... Step: 1310... Loss: 1.4584\n",
      "Epoch: 9/20... Step: 1320... Loss: 1.4297\n",
      "Epoch: 9/20... Step: 1330... Loss: 1.4437\n",
      "Epoch: 9/20... Step: 1340... Loss: 1.4525\n",
      "Epoch: 9/20... Step: 1350... Loss: 1.4641\n",
      "Epoch: 9/20... Step: 1360... Loss: 1.4370\n",
      "Epoch: 9/20... Step: 1370... Loss: 1.4266\n",
      "Epoch: 9/20... Step: 1380... Loss: 1.4526\n",
      "Epoch: 9/20... Step: 1390... Loss: 1.4349\n",
      "Epoch: 10/20... Step: 1400... Loss: 1.4036\n",
      "Epoch: 10/20... Step: 1410... Loss: 1.3875\n",
      "Epoch: 10/20... Step: 1420... Loss: 1.4153\n",
      "Epoch: 10/20... Step: 1430... Loss: 1.4285\n",
      "Epoch: 10/20... Step: 1440... Loss: 1.4191\n",
      "Epoch: 10/20... Step: 1450... Loss: 1.4138\n",
      "Epoch: 10/20... Step: 1460... Loss: 1.4270\n",
      "Epoch: 10/20... Step: 1470... Loss: 1.4387\n",
      "Epoch: 10/20... Step: 1480... Loss: 1.4154\n",
      "Epoch: 10/20... Step: 1490... Loss: 1.4013\n",
      "Epoch: 10/20... Step: 1500... Loss: 1.3920\n",
      "Epoch: 10/20... Step: 1510... Loss: 1.3912\n",
      "Epoch: 10/20... Step: 1520... Loss: 1.3813\n",
      "Epoch: 10/20... Step: 1530... Loss: 1.4022\n",
      "Epoch: 10/20... Step: 1540... Loss: 1.4287\n",
      "Epoch: 10/20... Step: 1550... Loss: 1.3683\n",
      "Epoch: 11/20... Step: 1560... Loss: 1.3898\n",
      "Epoch: 11/20... Step: 1570... Loss: 1.4020\n",
      "Epoch: 11/20... Step: 1580... Loss: 1.4091\n",
      "Epoch: 11/20... Step: 1590... Loss: 1.4092\n",
      "Epoch: 11/20... Step: 1600... Loss: 1.4128\n",
      "Epoch: 11/20... Step: 1610... Loss: 1.3675\n",
      "Epoch: 11/20... Step: 1620... Loss: 1.4090\n",
      "Epoch: 11/20... Step: 1630... Loss: 1.3778\n",
      "Epoch: 11/20... Step: 1640... Loss: 1.3984\n",
      "Epoch: 11/20... Step: 1650... Loss: 1.4086\n",
      "Epoch: 11/20... Step: 1660... Loss: 1.4181\n",
      "Epoch: 11/20... Step: 1670... Loss: 1.3964\n",
      "Epoch: 11/20... Step: 1680... Loss: 1.3775\n",
      "Epoch: 11/20... Step: 1690... Loss: 1.3967\n",
      "Epoch: 11/20... Step: 1700... Loss: 1.3911\n",
      "Epoch: 12/20... Step: 1710... Loss: 1.3570\n",
      "Epoch: 12/20... Step: 1720... Loss: 1.3551\n",
      "Epoch: 12/20... Step: 1730... Loss: 1.3721\n",
      "Epoch: 12/20... Step: 1740... Loss: 1.3806\n",
      "Epoch: 12/20... Step: 1750... Loss: 1.3687\n",
      "Epoch: 12/20... Step: 1760... Loss: 1.3660\n",
      "Epoch: 12/20... Step: 1770... Loss: 1.3891\n",
      "Epoch: 12/20... Step: 1780... Loss: 1.3862\n",
      "Epoch: 12/20... Step: 1790... Loss: 1.3749\n",
      "Epoch: 12/20... Step: 1800... Loss: 1.3679\n",
      "Epoch: 12/20... Step: 1810... Loss: 1.3508\n",
      "Epoch: 12/20... Step: 1820... Loss: 1.3442\n",
      "Epoch: 12/20... Step: 1830... Loss: 1.3477\n",
      "Epoch: 12/20... Step: 1840... Loss: 1.3596\n",
      "Epoch: 12/20... Step: 1850... Loss: 1.3878\n",
      "Epoch: 12/20... Step: 1860... Loss: 1.3275\n",
      "Epoch: 13/20... Step: 1870... Loss: 1.3600\n",
      "Epoch: 13/20... Step: 1880... Loss: 1.3660\n",
      "Epoch: 13/20... Step: 1890... Loss: 1.3683\n",
      "Epoch: 13/20... Step: 1900... Loss: 1.3619\n",
      "Epoch: 13/20... Step: 1910... Loss: 1.3713\n",
      "Epoch: 13/20... Step: 1920... Loss: 1.3362\n",
      "Epoch: 13/20... Step: 1930... Loss: 1.3674\n",
      "Epoch: 13/20... Step: 1940... Loss: 1.3422\n",
      "Epoch: 13/20... Step: 1950... Loss: 1.3463\n",
      "Epoch: 13/20... Step: 1960... Loss: 1.3733\n",
      "Epoch: 13/20... Step: 1970... Loss: 1.3838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20... Step: 1980... Loss: 1.3633\n",
      "Epoch: 13/20... Step: 1990... Loss: 1.3426\n",
      "Epoch: 13/20... Step: 2000... Loss: 1.3631\n",
      "Epoch: 13/20... Step: 2010... Loss: 1.3447\n",
      "Epoch: 14/20... Step: 2020... Loss: 1.3139\n",
      "Epoch: 14/20... Step: 2030... Loss: 1.3140\n",
      "Epoch: 14/20... Step: 2040... Loss: 1.3429\n",
      "Epoch: 14/20... Step: 2050... Loss: 1.3424\n",
      "Epoch: 14/20... Step: 2060... Loss: 1.3335\n",
      "Epoch: 14/20... Step: 2070... Loss: 1.3300\n",
      "Epoch: 14/20... Step: 2080... Loss: 1.3588\n",
      "Epoch: 14/20... Step: 2090... Loss: 1.3585\n",
      "Epoch: 14/20... Step: 2100... Loss: 1.3404\n",
      "Epoch: 14/20... Step: 2110... Loss: 1.3409\n",
      "Epoch: 14/20... Step: 2120... Loss: 1.3209\n",
      "Epoch: 14/20... Step: 2130... Loss: 1.3047\n",
      "Epoch: 14/20... Step: 2140... Loss: 1.3133\n",
      "Epoch: 14/20... Step: 2150... Loss: 1.3360\n",
      "Epoch: 14/20... Step: 2160... Loss: 1.3614\n",
      "Epoch: 14/20... Step: 2170... Loss: 1.3010\n",
      "Epoch: 15/20... Step: 2180... Loss: 1.3334\n",
      "Epoch: 15/20... Step: 2190... Loss: 1.3381\n",
      "Epoch: 15/20... Step: 2200... Loss: 1.3409\n",
      "Epoch: 15/20... Step: 2210... Loss: 1.3374\n",
      "Epoch: 15/20... Step: 2220... Loss: 1.3403\n",
      "Epoch: 15/20... Step: 2230... Loss: 1.2942\n",
      "Epoch: 15/20... Step: 2240... Loss: 1.3301\n",
      "Epoch: 15/20... Step: 2250... Loss: 1.3145\n",
      "Epoch: 15/20... Step: 2260... Loss: 1.3199\n",
      "Epoch: 15/20... Step: 2270... Loss: 1.3414\n",
      "Epoch: 15/20... Step: 2280... Loss: 1.3507\n",
      "Epoch: 15/20... Step: 2290... Loss: 1.3297\n",
      "Epoch: 15/20... Step: 2300... Loss: 1.3082\n",
      "Epoch: 15/20... Step: 2310... Loss: 1.3360\n",
      "Epoch: 15/20... Step: 2320... Loss: 1.3251\n",
      "Epoch: 16/20... Step: 2330... Loss: 1.2933\n",
      "Epoch: 16/20... Step: 2340... Loss: 1.2792\n",
      "Epoch: 16/20... Step: 2350... Loss: 1.3133\n",
      "Epoch: 16/20... Step: 2360... Loss: 1.3168\n",
      "Epoch: 16/20... Step: 2370... Loss: 1.3054\n",
      "Epoch: 16/20... Step: 2380... Loss: 1.3051\n",
      "Epoch: 16/20... Step: 2390... Loss: 1.3323\n",
      "Epoch: 16/20... Step: 2400... Loss: 1.3412\n",
      "Epoch: 16/20... Step: 2410... Loss: 1.3140\n",
      "Epoch: 16/20... Step: 2420... Loss: 1.3173\n",
      "Epoch: 16/20... Step: 2430... Loss: 1.3059\n",
      "Epoch: 16/20... Step: 2440... Loss: 1.2774\n",
      "Epoch: 16/20... Step: 2450... Loss: 1.2845\n",
      "Epoch: 16/20... Step: 2460... Loss: 1.3041\n",
      "Epoch: 16/20... Step: 2470... Loss: 1.3449\n",
      "Epoch: 16/20... Step: 2480... Loss: 1.2845\n",
      "Epoch: 17/20... Step: 2490... Loss: 1.3078\n",
      "Epoch: 17/20... Step: 2500... Loss: 1.3159\n",
      "Epoch: 17/20... Step: 2510... Loss: 1.3109\n",
      "Epoch: 17/20... Step: 2520... Loss: 1.3073\n",
      "Epoch: 17/20... Step: 2530... Loss: 1.3221\n",
      "Epoch: 17/20... Step: 2540... Loss: 1.2809\n",
      "Epoch: 17/20... Step: 2550... Loss: 1.3077\n",
      "Epoch: 17/20... Step: 2560... Loss: 1.2868\n",
      "Epoch: 17/20... Step: 2570... Loss: 1.3019\n",
      "Epoch: 17/20... Step: 2580... Loss: 1.3241\n",
      "Epoch: 17/20... Step: 2590... Loss: 1.3229\n",
      "Epoch: 17/20... Step: 2600... Loss: 1.3126\n",
      "Epoch: 17/20... Step: 2610... Loss: 1.2917\n",
      "Epoch: 17/20... Step: 2620... Loss: 1.3154\n",
      "Epoch: 17/20... Step: 2630... Loss: 1.2977\n",
      "Epoch: 18/20... Step: 2640... Loss: 1.2770\n",
      "Epoch: 18/20... Step: 2650... Loss: 1.2603\n",
      "Epoch: 18/20... Step: 2660... Loss: 1.3022\n",
      "Epoch: 18/20... Step: 2670... Loss: 1.3006\n",
      "Epoch: 18/20... Step: 2680... Loss: 1.2743\n",
      "Epoch: 18/20... Step: 2690... Loss: 1.2827\n",
      "Epoch: 18/20... Step: 2700... Loss: 1.3183\n",
      "Epoch: 18/20... Step: 2710... Loss: 1.3072\n",
      "Epoch: 18/20... Step: 2720... Loss: 1.2938\n",
      "Epoch: 18/20... Step: 2730... Loss: 1.2894\n",
      "Epoch: 18/20... Step: 2740... Loss: 1.2769\n",
      "Epoch: 18/20... Step: 2750... Loss: 1.2695\n",
      "Epoch: 18/20... Step: 2760... Loss: 1.2611\n",
      "Epoch: 18/20... Step: 2770... Loss: 1.2801\n",
      "Epoch: 18/20... Step: 2780... Loss: 1.3184\n",
      "Epoch: 18/20... Step: 2790... Loss: 1.2496\n",
      "Epoch: 19/20... Step: 2800... Loss: 1.2877\n",
      "Epoch: 19/20... Step: 2810... Loss: 1.2969\n",
      "Epoch: 19/20... Step: 2820... Loss: 1.2969\n",
      "Epoch: 19/20... Step: 2830... Loss: 1.2881\n",
      "Epoch: 19/20... Step: 2840... Loss: 1.2947\n",
      "Epoch: 19/20... Step: 2850... Loss: 1.2712\n",
      "Epoch: 19/20... Step: 2860... Loss: 1.2910\n",
      "Epoch: 19/20... Step: 2870... Loss: 1.2744\n",
      "Epoch: 19/20... Step: 2880... Loss: 1.2784\n",
      "Epoch: 19/20... Step: 2890... Loss: 1.2939\n",
      "Epoch: 19/20... Step: 2900... Loss: 1.3039\n",
      "Epoch: 19/20... Step: 2910... Loss: 1.2949\n",
      "Epoch: 19/20... Step: 2920... Loss: 1.2696\n",
      "Epoch: 19/20... Step: 2930... Loss: 1.2958\n",
      "Epoch: 19/20... Step: 2940... Loss: 1.2903\n",
      "Epoch: 20/20... Step: 2950... Loss: 1.2625\n",
      "Epoch: 20/20... Step: 2960... Loss: 1.2374\n",
      "Epoch: 20/20... Step: 2970... Loss: 1.2726\n",
      "Epoch: 20/20... Step: 2980... Loss: 1.2763\n",
      "Epoch: 20/20... Step: 2990... Loss: 1.2553\n",
      "Epoch: 20/20... Step: 3000... Loss: 1.2631\n",
      "Epoch: 20/20... Step: 3010... Loss: 1.2899\n",
      "Epoch: 20/20... Step: 3020... Loss: 1.3032\n",
      "Epoch: 20/20... Step: 3030... Loss: 1.2800\n",
      "Epoch: 20/20... Step: 3040... Loss: 1.2663\n",
      "Epoch: 20/20... Step: 3050... Loss: 1.2619\n",
      "Epoch: 20/20... Step: 3060... Loss: 1.2455\n",
      "Epoch: 20/20... Step: 3070... Loss: 1.2505\n",
      "Epoch: 20/20... Step: 3080... Loss: 1.2678\n",
      "Epoch: 20/20... Step: 3090... Loss: 1.2977\n",
      "Epoch: 20/20... Step: 3100... Loss: 1.2421\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, 20, n_seqs=n_seqs, n_steps=n_steps, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ch, _ = net.predict('g', top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char='A'\n",
    "h = net.init_hidden(1)\n",
    "x = np.array([[char2int[char]]])\n",
    "inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "\n",
    "h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "out, h = net.forward(inputs, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = F.softmax(out).data.numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p[np.argsort(p)[:-5]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "p = p/p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(len(chars), p=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annag thos tar and wind tor onther whe sartes hit sat ho wore he tasit ant hhate\n",
      "nhans.\n",
      "\"od har ant ter orotang se wort oud wonte har, ard wit he an serisde\n",
      "site te teasd he hate tiste whe terans he thes h\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 200, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
