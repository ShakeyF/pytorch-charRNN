{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN in PyTorch\n",
    "\n",
    "In this notebook, I'll construct a character-level RNN with PyTorch. If you are unfamiliar with character-level RNNs, check out [this great article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina, one of my favorite novels. I call this project Anna KaRNNa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the text, encode it as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "\n",
    "We're one-hot encoding the data, so I'll make a function to do that.\n",
    "\n",
    "I'll also create mini-batches for training. We'll take the encoded characters and split them into multiple sequences, given by `n_seqs` (also refered to as \"batch size\" in other places). Each of those sequences will be `n_steps` long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the network with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to define the architecture of the network. We start by defining the layers and operations we want. Then, define a method for the forward pass. I'm also going to write a method for predicting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y, volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Now we can actually train the network. First we'll create the network itself, with some given hyperparameters. Then, define the mini-batches sizes (number of sequences and number of steps), and start the training. With the train function, we can set the number of epochs, the learning rate, and other parameters. Also, we can run the training on a GPU by setting `cuda=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3013... Val Loss: 3.2742\n",
      "Epoch: 1/25... Step: 20... Loss: 3.1536... Val Loss: 3.1649\n",
      "Epoch: 1/25... Step: 30... Loss: 3.0519... Val Loss: 3.0337\n",
      "Epoch: 1/25... Step: 40... Loss: 2.8571... Val Loss: 2.8676\n",
      "Epoch: 1/25... Step: 50... Loss: 2.7270... Val Loss: 2.7022\n",
      "Epoch: 1/25... Step: 60... Loss: 2.5928... Val Loss: 2.5938\n",
      "Epoch: 1/25... Step: 70... Loss: 2.5080... Val Loss: 2.5275\n",
      "Epoch: 1/25... Step: 80... Loss: 2.4407... Val Loss: 2.4770\n",
      "Epoch: 1/25... Step: 90... Loss: 2.4260... Val Loss: 2.4359\n",
      "Epoch: 1/25... Step: 100... Loss: 2.3660... Val Loss: 2.3986\n",
      "Epoch: 1/25... Step: 110... Loss: 2.3305... Val Loss: 2.3701\n",
      "Epoch: 1/25... Step: 120... Loss: 2.2597... Val Loss: 2.3454\n",
      "Epoch: 1/25... Step: 130... Loss: 2.2854... Val Loss: 2.3329\n",
      "Epoch: 2/25... Step: 140... Loss: 2.2432... Val Loss: 2.2891\n",
      "Epoch: 2/25... Step: 150... Loss: 2.2223... Val Loss: 2.2631\n",
      "Epoch: 2/25... Step: 160... Loss: 2.2183... Val Loss: 2.2418\n",
      "Epoch: 2/25... Step: 170... Loss: 2.1766... Val Loss: 2.2101\n",
      "Epoch: 2/25... Step: 180... Loss: 2.1230... Val Loss: 2.1944\n",
      "Epoch: 2/25... Step: 190... Loss: 2.0650... Val Loss: 2.1714\n",
      "Epoch: 2/25... Step: 200... Loss: 2.0753... Val Loss: 2.1529\n",
      "Epoch: 2/25... Step: 210... Loss: 2.0743... Val Loss: 2.1298\n",
      "Epoch: 2/25... Step: 220... Loss: 2.0302... Val Loss: 2.1106\n",
      "Epoch: 2/25... Step: 230... Loss: 2.0428... Val Loss: 2.0976\n",
      "Epoch: 2/25... Step: 240... Loss: 2.0202... Val Loss: 2.0844\n",
      "Epoch: 2/25... Step: 250... Loss: 1.9719... Val Loss: 2.0754\n",
      "Epoch: 2/25... Step: 260... Loss: 1.9401... Val Loss: 2.0487\n",
      "Epoch: 2/25... Step: 270... Loss: 1.9689... Val Loss: 2.0326\n",
      "Epoch: 3/25... Step: 280... Loss: 1.9590... Val Loss: 2.0205\n",
      "Epoch: 3/25... Step: 290... Loss: 1.9522... Val Loss: 2.0043\n",
      "Epoch: 3/25... Step: 300... Loss: 1.9132... Val Loss: 1.9926\n",
      "Epoch: 3/25... Step: 310... Loss: 1.8925... Val Loss: 1.9782\n",
      "Epoch: 3/25... Step: 320... Loss: 1.8712... Val Loss: 1.9756\n",
      "Epoch: 3/25... Step: 330... Loss: 1.8570... Val Loss: 1.9599\n",
      "Epoch: 3/25... Step: 340... Loss: 1.8967... Val Loss: 1.9570\n",
      "Epoch: 3/25... Step: 350... Loss: 1.8561... Val Loss: 1.9376\n",
      "Epoch: 3/25... Step: 360... Loss: 1.7999... Val Loss: 1.9310\n",
      "Epoch: 3/25... Step: 370... Loss: 1.8367... Val Loss: 1.9210\n",
      "Epoch: 3/25... Step: 380... Loss: 1.8311... Val Loss: 1.9166\n",
      "Epoch: 3/25... Step: 390... Loss: 1.7985... Val Loss: 1.9049\n",
      "Epoch: 3/25... Step: 400... Loss: 1.7756... Val Loss: 1.8858\n",
      "Epoch: 3/25... Step: 410... Loss: 1.7911... Val Loss: 1.8842\n",
      "Epoch: 4/25... Step: 420... Loss: 1.7849... Val Loss: 1.8727\n",
      "Epoch: 4/25... Step: 430... Loss: 1.7769... Val Loss: 1.8580\n",
      "Epoch: 4/25... Step: 440... Loss: 1.7652... Val Loss: 1.8480\n",
      "Epoch: 4/25... Step: 450... Loss: 1.7140... Val Loss: 1.8447\n",
      "Epoch: 4/25... Step: 460... Loss: 1.7045... Val Loss: 1.8481\n",
      "Epoch: 4/25... Step: 470... Loss: 1.7575... Val Loss: 1.8310\n",
      "Epoch: 4/25... Step: 480... Loss: 1.7247... Val Loss: 1.8189\n",
      "Epoch: 4/25... Step: 490... Loss: 1.7346... Val Loss: 1.8102\n",
      "Epoch: 4/25... Step: 500... Loss: 1.7266... Val Loss: 1.8004\n",
      "Epoch: 4/25... Step: 510... Loss: 1.6941... Val Loss: 1.7945\n",
      "Epoch: 4/25... Step: 520... Loss: 1.7140... Val Loss: 1.7884\n",
      "Epoch: 4/25... Step: 530... Loss: 1.6693... Val Loss: 1.8037\n",
      "Epoch: 4/25... Step: 540... Loss: 1.6404... Val Loss: 1.7673\n",
      "Epoch: 4/25... Step: 550... Loss: 1.7013... Val Loss: 1.7646\n",
      "Epoch: 5/25... Step: 560... Loss: 1.6599... Val Loss: 1.7618\n",
      "Epoch: 5/25... Step: 570... Loss: 1.6613... Val Loss: 1.7545\n",
      "Epoch: 5/25... Step: 580... Loss: 1.6392... Val Loss: 1.7503\n",
      "Epoch: 5/25... Step: 590... Loss: 1.6333... Val Loss: 1.7483\n",
      "Epoch: 5/25... Step: 600... Loss: 1.6235... Val Loss: 1.7441\n",
      "Epoch: 5/25... Step: 610... Loss: 1.5986... Val Loss: 1.7267\n",
      "Epoch: 5/25... Step: 620... Loss: 1.6301... Val Loss: 1.7249\n",
      "Epoch: 5/25... Step: 630... Loss: 1.6285... Val Loss: 1.7233\n",
      "Epoch: 5/25... Step: 640... Loss: 1.5947... Val Loss: 1.7160\n",
      "Epoch: 5/25... Step: 650... Loss: 1.5998... Val Loss: 1.7146\n",
      "Epoch: 5/25... Step: 660... Loss: 1.5859... Val Loss: 1.7080\n",
      "Epoch: 5/25... Step: 670... Loss: 1.6024... Val Loss: 1.7070\n",
      "Epoch: 5/25... Step: 680... Loss: 1.5948... Val Loss: 1.6884\n",
      "Epoch: 5/25... Step: 690... Loss: 1.5655... Val Loss: 1.6925\n",
      "Epoch: 6/25... Step: 700... Loss: 1.5776... Val Loss: 1.6847\n",
      "Epoch: 6/25... Step: 710... Loss: 1.5619... Val Loss: 1.6837\n",
      "Epoch: 6/25... Step: 720... Loss: 1.5576... Val Loss: 1.6804\n",
      "Epoch: 6/25... Step: 730... Loss: 1.5816... Val Loss: 1.6765\n",
      "Epoch: 6/25... Step: 740... Loss: 1.5345... Val Loss: 1.6676\n",
      "Epoch: 6/25... Step: 750... Loss: 1.5254... Val Loss: 1.6747\n",
      "Epoch: 6/25... Step: 760... Loss: 1.5707... Val Loss: 1.6717\n",
      "Epoch: 6/25... Step: 770... Loss: 1.5474... Val Loss: 1.6601\n",
      "Epoch: 6/25... Step: 780... Loss: 1.5247... Val Loss: 1.6530\n",
      "Epoch: 6/25... Step: 790... Loss: 1.5042... Val Loss: 1.6441\n",
      "Epoch: 6/25... Step: 800... Loss: 1.5363... Val Loss: 1.6431\n",
      "Epoch: 6/25... Step: 810... Loss: 1.5098... Val Loss: 1.6378\n",
      "Epoch: 6/25... Step: 820... Loss: 1.4751... Val Loss: 1.6387\n",
      "Epoch: 6/25... Step: 830... Loss: 1.5200... Val Loss: 1.6370\n",
      "Epoch: 7/25... Step: 840... Loss: 1.4770... Val Loss: 1.6283\n",
      "Epoch: 7/25... Step: 850... Loss: 1.4988... Val Loss: 1.6290\n",
      "Epoch: 7/25... Step: 860... Loss: 1.4732... Val Loss: 1.6263\n",
      "Epoch: 7/25... Step: 870... Loss: 1.4912... Val Loss: 1.6261\n",
      "Epoch: 7/25... Step: 880... Loss: 1.4965... Val Loss: 1.6149\n",
      "Epoch: 7/25... Step: 890... Loss: 1.4913... Val Loss: 1.6147\n",
      "Epoch: 7/25... Step: 900... Loss: 1.4871... Val Loss: 1.6180\n",
      "Epoch: 7/25... Step: 910... Loss: 1.4546... Val Loss: 1.6051\n",
      "Epoch: 7/25... Step: 920... Loss: 1.4691... Val Loss: 1.6030\n",
      "Epoch: 7/25... Step: 930... Loss: 1.4524... Val Loss: 1.6127\n",
      "Epoch: 7/25... Step: 940... Loss: 1.4569... Val Loss: 1.6041\n",
      "Epoch: 7/25... Step: 950... Loss: 1.4710... Val Loss: 1.5948\n",
      "Epoch: 7/25... Step: 960... Loss: 1.4832... Val Loss: 1.5856\n",
      "Epoch: 7/25... Step: 970... Loss: 1.4819... Val Loss: 1.5868\n",
      "Epoch: 8/25... Step: 980... Loss: 1.4376... Val Loss: 1.5938\n",
      "Epoch: 8/25... Step: 990... Loss: 1.4499... Val Loss: 1.5825\n",
      "Epoch: 8/25... Step: 1000... Loss: 1.4533... Val Loss: 1.5773\n",
      "Epoch: 8/25... Step: 1010... Loss: 1.4710... Val Loss: 1.5823\n",
      "Epoch: 8/25... Step: 1020... Loss: 1.4581... Val Loss: 1.5728\n",
      "Epoch: 8/25... Step: 1030... Loss: 1.4301... Val Loss: 1.5696\n",
      "Epoch: 8/25... Step: 1040... Loss: 1.4448... Val Loss: 1.5744\n",
      "Epoch: 8/25... Step: 1050... Loss: 1.4268... Val Loss: 1.5709\n",
      "Epoch: 8/25... Step: 1060... Loss: 1.4172... Val Loss: 1.5630\n",
      "Epoch: 8/25... Step: 1070... Loss: 1.4315... Val Loss: 1.5592\n",
      "Epoch: 8/25... Step: 1080... Loss: 1.4224... Val Loss: 1.5601\n",
      "Epoch: 8/25... Step: 1090... Loss: 1.4088... Val Loss: 1.5542\n",
      "Epoch: 8/25... Step: 1100... Loss: 1.4145... Val Loss: 1.5488\n",
      "Epoch: 8/25... Step: 1110... Loss: 1.4105... Val Loss: 1.5577\n",
      "Epoch: 9/25... Step: 1120... Loss: 1.4220... Val Loss: 1.5504\n",
      "Epoch: 9/25... Step: 1130... Loss: 1.4280... Val Loss: 1.5539\n",
      "Epoch: 9/25... Step: 1140... Loss: 1.4255... Val Loss: 1.5439\n",
      "Epoch: 9/25... Step: 1150... Loss: 1.4430... Val Loss: 1.5513\n",
      "Epoch: 9/25... Step: 1160... Loss: 1.3930... Val Loss: 1.5404\n",
      "Epoch: 9/25... Step: 1170... Loss: 1.4035... Val Loss: 1.5408\n",
      "Epoch: 9/25... Step: 1180... Loss: 1.3888... Val Loss: 1.5346\n",
      "Epoch: 9/25... Step: 1190... Loss: 1.4341... Val Loss: 1.5332\n",
      "Epoch: 9/25... Step: 1200... Loss: 1.3833... Val Loss: 1.5371\n",
      "Epoch: 9/25... Step: 1210... Loss: 1.3706... Val Loss: 1.5468\n",
      "Epoch: 9/25... Step: 1220... Loss: 1.3948... Val Loss: 1.5359\n",
      "Epoch: 9/25... Step: 1230... Loss: 1.3641... Val Loss: 1.5279\n",
      "Epoch: 9/25... Step: 1240... Loss: 1.3758... Val Loss: 1.5243\n",
      "Epoch: 9/25... Step: 1250... Loss: 1.3796... Val Loss: 1.5247\n",
      "Epoch: 10/25... Step: 1260... Loss: 1.3912... Val Loss: 1.5243\n",
      "Epoch: 10/25... Step: 1270... Loss: 1.3846... Val Loss: 1.5230\n",
      "Epoch: 10/25... Step: 1280... Loss: 1.3905... Val Loss: 1.5219\n",
      "Epoch: 10/25... Step: 1290... Loss: 1.3758... Val Loss: 1.5233\n",
      "Epoch: 10/25... Step: 1300... Loss: 1.3740... Val Loss: 1.5256\n",
      "Epoch: 10/25... Step: 1310... Loss: 1.3720... Val Loss: 1.5139\n",
      "Epoch: 10/25... Step: 1320... Loss: 1.3459... Val Loss: 1.5201\n",
      "Epoch: 10/25... Step: 1330... Loss: 1.3611... Val Loss: 1.5157\n",
      "Epoch: 10/25... Step: 1340... Loss: 1.3476... Val Loss: 1.5151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25... Step: 1350... Loss: 1.3363... Val Loss: 1.5153\n",
      "Epoch: 10/25... Step: 1360... Loss: 1.3492... Val Loss: 1.5102\n",
      "Epoch: 10/25... Step: 1370... Loss: 1.3370... Val Loss: 1.5151\n",
      "Epoch: 10/25... Step: 1380... Loss: 1.3727... Val Loss: 1.5144\n",
      "Epoch: 10/25... Step: 1390... Loss: 1.3911... Val Loss: 1.5059\n",
      "Epoch: 11/25... Step: 1400... Loss: 1.3881... Val Loss: 1.5252\n",
      "Epoch: 11/25... Step: 1410... Loss: 1.4026... Val Loss: 1.5044\n",
      "Epoch: 11/25... Step: 1420... Loss: 1.3797... Val Loss: 1.4943\n",
      "Epoch: 11/25... Step: 1430... Loss: 1.3480... Val Loss: 1.4947\n",
      "Epoch: 11/25... Step: 1440... Loss: 1.3691... Val Loss: 1.4994\n",
      "Epoch: 11/25... Step: 1450... Loss: 1.3095... Val Loss: 1.4916\n",
      "Epoch: 11/25... Step: 1460... Loss: 1.3372... Val Loss: 1.4909\n",
      "Epoch: 11/25... Step: 1470... Loss: 1.3192... Val Loss: 1.4959\n",
      "Epoch: 11/25... Step: 1480... Loss: 1.3411... Val Loss: 1.4926\n",
      "Epoch: 11/25... Step: 1490... Loss: 1.3306... Val Loss: 1.4917\n",
      "Epoch: 11/25... Step: 1500... Loss: 1.3174... Val Loss: 1.4903\n",
      "Epoch: 11/25... Step: 1510... Loss: 1.3067... Val Loss: 1.4932\n",
      "Epoch: 11/25... Step: 1520... Loss: 1.3322... Val Loss: 1.4898\n",
      "Epoch: 12/25... Step: 1530... Loss: 1.3816... Val Loss: 1.4912\n",
      "Epoch: 12/25... Step: 1540... Loss: 1.3461... Val Loss: 1.4851\n",
      "Epoch: 12/25... Step: 1550... Loss: 1.3525... Val Loss: 1.4802\n",
      "Epoch: 12/25... Step: 1560... Loss: 1.3546... Val Loss: 1.4762\n",
      "Epoch: 12/25... Step: 1570... Loss: 1.3022... Val Loss: 1.4833\n",
      "Epoch: 12/25... Step: 1580... Loss: 1.2907... Val Loss: 1.4994\n",
      "Epoch: 12/25... Step: 1590... Loss: 1.2764... Val Loss: 1.4826\n",
      "Epoch: 12/25... Step: 1600... Loss: 1.3131... Val Loss: 1.4829\n",
      "Epoch: 12/25... Step: 1610... Loss: 1.2992... Val Loss: 1.4802\n",
      "Epoch: 12/25... Step: 1620... Loss: 1.2949... Val Loss: 1.4773\n",
      "Epoch: 12/25... Step: 1630... Loss: 1.3225... Val Loss: 1.4805\n",
      "Epoch: 12/25... Step: 1640... Loss: 1.3076... Val Loss: 1.4829\n",
      "Epoch: 12/25... Step: 1650... Loss: 1.2841... Val Loss: 1.4708\n",
      "Epoch: 12/25... Step: 1660... Loss: 1.3370... Val Loss: 1.4702\n",
      "Epoch: 13/25... Step: 1670... Loss: 1.3038... Val Loss: 1.4745\n",
      "Epoch: 13/25... Step: 1680... Loss: 1.3207... Val Loss: 1.4782\n",
      "Epoch: 13/25... Step: 1690... Loss: 1.2946... Val Loss: 1.4686\n",
      "Epoch: 13/25... Step: 1700... Loss: 1.2889... Val Loss: 1.4646\n",
      "Epoch: 13/25... Step: 1710... Loss: 1.2811... Val Loss: 1.4732\n",
      "Epoch: 13/25... Step: 1720... Loss: 1.2834... Val Loss: 1.4705\n",
      "Epoch: 13/25... Step: 1730... Loss: 1.3203... Val Loss: 1.4609\n",
      "Epoch: 13/25... Step: 1740... Loss: 1.2857... Val Loss: 1.4627\n",
      "Epoch: 13/25... Step: 1750... Loss: 1.2581... Val Loss: 1.4702\n",
      "Epoch: 13/25... Step: 1760... Loss: 1.2939... Val Loss: 1.4669\n",
      "Epoch: 13/25... Step: 1770... Loss: 1.3060... Val Loss: 1.4612\n",
      "Epoch: 13/25... Step: 1780... Loss: 1.2789... Val Loss: 1.4667\n",
      "Epoch: 13/25... Step: 1790... Loss: 1.2711... Val Loss: 1.4614\n",
      "Epoch: 13/25... Step: 1800... Loss: 1.2862... Val Loss: 1.4587\n",
      "Epoch: 14/25... Step: 1810... Loss: 1.2981... Val Loss: 1.4665\n",
      "Epoch: 14/25... Step: 1820... Loss: 1.2810... Val Loss: 1.4625\n",
      "Epoch: 14/25... Step: 1830... Loss: 1.3095... Val Loss: 1.4520\n",
      "Epoch: 14/25... Step: 1840... Loss: 1.2554... Val Loss: 1.4531\n",
      "Epoch: 14/25... Step: 1850... Loss: 1.2328... Val Loss: 1.4610\n",
      "Epoch: 14/25... Step: 1860... Loss: 1.2920... Val Loss: 1.4643\n",
      "Epoch: 14/25... Step: 1870... Loss: 1.2888... Val Loss: 1.4586\n",
      "Epoch: 14/25... Step: 1880... Loss: 1.2793... Val Loss: 1.4574\n",
      "Epoch: 14/25... Step: 1890... Loss: 1.2974... Val Loss: 1.4574\n",
      "Epoch: 14/25... Step: 1900... Loss: 1.2839... Val Loss: 1.4537\n",
      "Epoch: 14/25... Step: 1910... Loss: 1.2777... Val Loss: 1.4496\n",
      "Epoch: 14/25... Step: 1920... Loss: 1.2822... Val Loss: 1.4530\n",
      "Epoch: 14/25... Step: 1930... Loss: 1.2429... Val Loss: 1.4500\n",
      "Epoch: 14/25... Step: 1940... Loss: 1.2963... Val Loss: 1.4441\n",
      "Epoch: 15/25... Step: 1950... Loss: 1.2756... Val Loss: 1.4501\n",
      "Epoch: 15/25... Step: 1960... Loss: 1.2675... Val Loss: 1.4561\n",
      "Epoch: 15/25... Step: 1970... Loss: 1.2649... Val Loss: 1.4481\n",
      "Epoch: 15/25... Step: 1980... Loss: 1.2612... Val Loss: 1.4555\n",
      "Epoch: 15/25... Step: 1990... Loss: 1.2471... Val Loss: 1.4524\n",
      "Epoch: 15/25... Step: 2000... Loss: 1.2427... Val Loss: 1.4587\n",
      "Epoch: 15/25... Step: 2010... Loss: 1.2687... Val Loss: 1.4435\n",
      "Epoch: 15/25... Step: 2020... Loss: 1.2832... Val Loss: 1.4423\n",
      "Epoch: 15/25... Step: 2030... Loss: 1.2430... Val Loss: 1.4512\n",
      "Epoch: 15/25... Step: 2040... Loss: 1.2645... Val Loss: 1.4365\n",
      "Epoch: 15/25... Step: 2050... Loss: 1.2482... Val Loss: 1.4369\n",
      "Epoch: 15/25... Step: 2060... Loss: 1.2544... Val Loss: 1.4444\n",
      "Epoch: 15/25... Step: 2070... Loss: 1.2628... Val Loss: 1.4371\n",
      "Epoch: 15/25... Step: 2080... Loss: 1.2621... Val Loss: 1.4364\n",
      "Epoch: 16/25... Step: 2090... Loss: 1.2634... Val Loss: 1.4507\n",
      "Epoch: 16/25... Step: 2100... Loss: 1.2520... Val Loss: 1.4468\n",
      "Epoch: 16/25... Step: 2110... Loss: 1.2447... Val Loss: 1.4402\n",
      "Epoch: 16/25... Step: 2120... Loss: 1.2554... Val Loss: 1.4413\n",
      "Epoch: 16/25... Step: 2130... Loss: 1.2289... Val Loss: 1.4357\n",
      "Epoch: 16/25... Step: 2140... Loss: 1.2382... Val Loss: 1.4368\n",
      "Epoch: 16/25... Step: 2150... Loss: 1.2637... Val Loss: 1.4396\n",
      "Epoch: 16/25... Step: 2160... Loss: 1.2342... Val Loss: 1.4345\n",
      "Epoch: 16/25... Step: 2170... Loss: 1.2382... Val Loss: 1.4378\n",
      "Epoch: 16/25... Step: 2180... Loss: 1.2284... Val Loss: 1.4417\n",
      "Epoch: 16/25... Step: 2190... Loss: 1.2639... Val Loss: 1.4357\n",
      "Epoch: 16/25... Step: 2200... Loss: 1.2290... Val Loss: 1.4370\n",
      "Epoch: 16/25... Step: 2210... Loss: 1.2099... Val Loss: 1.4226\n",
      "Epoch: 16/25... Step: 2220... Loss: 1.2511... Val Loss: 1.4250\n",
      "Epoch: 17/25... Step: 2230... Loss: 1.2177... Val Loss: 1.4306\n",
      "Epoch: 17/25... Step: 2240... Loss: 1.2301... Val Loss: 1.4363\n",
      "Epoch: 17/25... Step: 2250... Loss: 1.2223... Val Loss: 1.4310\n",
      "Epoch: 17/25... Step: 2260... Loss: 1.2176... Val Loss: 1.4429\n",
      "Epoch: 17/25... Step: 2270... Loss: 1.2369... Val Loss: 1.4293\n",
      "Epoch: 17/25... Step: 2280... Loss: 1.2441... Val Loss: 1.4268\n",
      "Epoch: 17/25... Step: 2290... Loss: 1.2384... Val Loss: 1.4242\n",
      "Epoch: 17/25... Step: 2300... Loss: 1.2100... Val Loss: 1.4298\n",
      "Epoch: 17/25... Step: 2310... Loss: 1.2328... Val Loss: 1.4247\n",
      "Epoch: 17/25... Step: 2320... Loss: 1.2144... Val Loss: 1.4213\n",
      "Epoch: 17/25... Step: 2330... Loss: 1.2245... Val Loss: 1.4231\n",
      "Epoch: 17/25... Step: 2340... Loss: 1.2368... Val Loss: 1.4264\n",
      "Epoch: 17/25... Step: 2350... Loss: 1.2406... Val Loss: 1.4197\n",
      "Epoch: 17/25... Step: 2360... Loss: 1.2405... Val Loss: 1.4217\n",
      "Epoch: 18/25... Step: 2370... Loss: 1.2150... Val Loss: 1.4253\n",
      "Epoch: 18/25... Step: 2380... Loss: 1.2236... Val Loss: 1.4318\n",
      "Epoch: 18/25... Step: 2390... Loss: 1.2319... Val Loss: 1.4246\n",
      "Epoch: 18/25... Step: 2400... Loss: 1.2324... Val Loss: 1.4176\n",
      "Epoch: 18/25... Step: 2410... Loss: 1.2373... Val Loss: 1.4126\n",
      "Epoch: 18/25... Step: 2420... Loss: 1.2153... Val Loss: 1.4211\n",
      "Epoch: 18/25... Step: 2430... Loss: 1.2227... Val Loss: 1.4210\n",
      "Epoch: 18/25... Step: 2440... Loss: 1.2158... Val Loss: 1.4139\n",
      "Epoch: 18/25... Step: 2450... Loss: 1.1900... Val Loss: 1.4152\n",
      "Epoch: 18/25... Step: 2460... Loss: 1.2132... Val Loss: 1.4128\n",
      "Epoch: 18/25... Step: 2470... Loss: 1.2085... Val Loss: 1.4138\n",
      "Epoch: 18/25... Step: 2480... Loss: 1.2028... Val Loss: 1.4168\n",
      "Epoch: 18/25... Step: 2490... Loss: 1.1957... Val Loss: 1.4073\n",
      "Epoch: 18/25... Step: 2500... Loss: 1.2003... Val Loss: 1.4069\n",
      "Epoch: 19/25... Step: 2510... Loss: 1.2153... Val Loss: 1.4155\n",
      "Epoch: 19/25... Step: 2520... Loss: 1.2186... Val Loss: 1.4238\n",
      "Epoch: 19/25... Step: 2530... Loss: 1.2309... Val Loss: 1.4089\n",
      "Epoch: 19/25... Step: 2540... Loss: 1.2343... Val Loss: 1.4141\n",
      "Epoch: 19/25... Step: 2550... Loss: 1.1925... Val Loss: 1.4149\n",
      "Epoch: 19/25... Step: 2560... Loss: 1.2198... Val Loss: 1.4070\n",
      "Epoch: 19/25... Step: 2570... Loss: 1.1867... Val Loss: 1.4060\n",
      "Epoch: 19/25... Step: 2580... Loss: 1.2279... Val Loss: 1.4120\n",
      "Epoch: 19/25... Step: 2590... Loss: 1.1860... Val Loss: 1.4103\n",
      "Epoch: 19/25... Step: 2600... Loss: 1.1851... Val Loss: 1.4033\n",
      "Epoch: 19/25... Step: 2610... Loss: 1.1984... Val Loss: 1.4042\n",
      "Epoch: 19/25... Step: 2620... Loss: 1.1776... Val Loss: 1.4107\n",
      "Epoch: 19/25... Step: 2630... Loss: 1.1785... Val Loss: 1.4031\n",
      "Epoch: 19/25... Step: 2640... Loss: 1.2027... Val Loss: 1.4014\n",
      "Epoch: 20/25... Step: 2650... Loss: 1.2042... Val Loss: 1.4158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25... Step: 2660... Loss: 1.2094... Val Loss: 1.4104\n",
      "Epoch: 20/25... Step: 2670... Loss: 1.2156... Val Loss: 1.4074\n",
      "Epoch: 20/25... Step: 2680... Loss: 1.1991... Val Loss: 1.4051\n",
      "Epoch: 20/25... Step: 2690... Loss: 1.2061... Val Loss: 1.4122\n",
      "Epoch: 20/25... Step: 2700... Loss: 1.1998... Val Loss: 1.4078\n",
      "Epoch: 20/25... Step: 2710... Loss: 1.1771... Val Loss: 1.3988\n",
      "Epoch: 20/25... Step: 2720... Loss: 1.1734... Val Loss: 1.4010\n",
      "Epoch: 20/25... Step: 2730... Loss: 1.1697... Val Loss: 1.4024\n",
      "Epoch: 20/25... Step: 2740... Loss: 1.1724... Val Loss: 1.3978\n",
      "Epoch: 20/25... Step: 2750... Loss: 1.1736... Val Loss: 1.4030\n",
      "Epoch: 20/25... Step: 2760... Loss: 1.1678... Val Loss: 1.4078\n",
      "Epoch: 20/25... Step: 2770... Loss: 1.2091... Val Loss: 1.3924\n",
      "Epoch: 20/25... Step: 2780... Loss: 1.2374... Val Loss: 1.3967\n",
      "Epoch: 21/25... Step: 2790... Loss: 1.2089... Val Loss: 1.4063\n",
      "Epoch: 21/25... Step: 2800... Loss: 1.2120... Val Loss: 1.3993\n",
      "Epoch: 21/25... Step: 2810... Loss: 1.2238... Val Loss: 1.3935\n",
      "Epoch: 21/25... Step: 2820... Loss: 1.1834... Val Loss: 1.3991\n",
      "Epoch: 21/25... Step: 2830... Loss: 1.1965... Val Loss: 1.3999\n",
      "Epoch: 21/25... Step: 2840... Loss: 1.1504... Val Loss: 1.3963\n",
      "Epoch: 21/25... Step: 2850... Loss: 1.1784... Val Loss: 1.3950\n",
      "Epoch: 21/25... Step: 2860... Loss: 1.1483... Val Loss: 1.3981\n",
      "Epoch: 21/25... Step: 2870... Loss: 1.1802... Val Loss: 1.3912\n",
      "Epoch: 21/25... Step: 2880... Loss: 1.1713... Val Loss: 1.3924\n",
      "Epoch: 21/25... Step: 2890... Loss: 1.1644... Val Loss: 1.3945\n",
      "Epoch: 21/25... Step: 2900... Loss: 1.1511... Val Loss: 1.3986\n",
      "Epoch: 21/25... Step: 2910... Loss: 1.1708... Val Loss: 1.3883\n",
      "Epoch: 22/25... Step: 2920... Loss: 1.2649... Val Loss: 1.3881\n",
      "Epoch: 22/25... Step: 2930... Loss: 1.1888... Val Loss: 1.3909\n",
      "Epoch: 22/25... Step: 2940... Loss: 1.1959... Val Loss: 1.3859\n",
      "Epoch: 22/25... Step: 2950... Loss: 1.2094... Val Loss: 1.3874\n",
      "Epoch: 22/25... Step: 2960... Loss: 1.1660... Val Loss: 1.3970\n",
      "Epoch: 22/25... Step: 2970... Loss: 1.1493... Val Loss: 1.3961\n",
      "Epoch: 22/25... Step: 2980... Loss: 1.1360... Val Loss: 1.3879\n",
      "Epoch: 22/25... Step: 2990... Loss: 1.1637... Val Loss: 1.3879\n",
      "Epoch: 22/25... Step: 3000... Loss: 1.1447... Val Loss: 1.3963\n",
      "Epoch: 22/25... Step: 3010... Loss: 1.1492... Val Loss: 1.3970\n",
      "Epoch: 22/25... Step: 3020... Loss: 1.1749... Val Loss: 1.3900\n",
      "Epoch: 22/25... Step: 3030... Loss: 1.1604... Val Loss: 1.3998\n",
      "Epoch: 22/25... Step: 3040... Loss: 1.1389... Val Loss: 1.4008\n",
      "Epoch: 22/25... Step: 3050... Loss: 1.1884... Val Loss: 1.3826\n",
      "Epoch: 23/25... Step: 3060... Loss: 1.1572... Val Loss: 1.3872\n",
      "Epoch: 23/25... Step: 3070... Loss: 1.1660... Val Loss: 1.3883\n",
      "Epoch: 23/25... Step: 3080... Loss: 1.1555... Val Loss: 1.3956\n",
      "Epoch: 23/25... Step: 3090... Loss: 1.1565... Val Loss: 1.3919\n",
      "Epoch: 23/25... Step: 3100... Loss: 1.1420... Val Loss: 1.3946\n",
      "Epoch: 23/25... Step: 3110... Loss: 1.1294... Val Loss: 1.3961\n",
      "Epoch: 23/25... Step: 3120... Loss: 1.1703... Val Loss: 1.3903\n",
      "Epoch: 23/25... Step: 3130... Loss: 1.1486... Val Loss: 1.3862\n",
      "Epoch: 23/25... Step: 3140... Loss: 1.1240... Val Loss: 1.3989\n",
      "Epoch: 23/25... Step: 3150... Loss: 1.1441... Val Loss: 1.4007\n",
      "Epoch: 23/25... Step: 3160... Loss: 1.1643... Val Loss: 1.3915\n",
      "Epoch: 23/25... Step: 3170... Loss: 1.1432... Val Loss: 1.3888\n",
      "Epoch: 23/25... Step: 3180... Loss: 1.1301... Val Loss: 1.3928\n",
      "Epoch: 23/25... Step: 3190... Loss: 1.1604... Val Loss: 1.3781\n",
      "Epoch: 24/25... Step: 3200... Loss: 1.1618... Val Loss: 1.3861\n",
      "Epoch: 24/25... Step: 3210... Loss: 1.1512... Val Loss: 1.3918\n",
      "Epoch: 24/25... Step: 3220... Loss: 1.1671... Val Loss: 1.3864\n",
      "Epoch: 24/25... Step: 3230... Loss: 1.1134... Val Loss: 1.3878\n",
      "Epoch: 24/25... Step: 3240... Loss: 1.1119... Val Loss: 1.3883\n",
      "Epoch: 24/25... Step: 3250... Loss: 1.1615... Val Loss: 1.3844\n",
      "Epoch: 24/25... Step: 3260... Loss: 1.1587... Val Loss: 1.3866\n",
      "Epoch: 24/25... Step: 3270... Loss: 1.1686... Val Loss: 1.3828\n",
      "Epoch: 24/25... Step: 3280... Loss: 1.1631... Val Loss: 1.3951\n",
      "Epoch: 24/25... Step: 3290... Loss: 1.1544... Val Loss: 1.3912\n",
      "Epoch: 24/25... Step: 3300... Loss: 1.1459... Val Loss: 1.3897\n",
      "Epoch: 24/25... Step: 3310... Loss: 1.1467... Val Loss: 1.3894\n",
      "Epoch: 24/25... Step: 3320... Loss: 1.1135... Val Loss: 1.3917\n",
      "Epoch: 24/25... Step: 3330... Loss: 1.1637... Val Loss: 1.3746\n",
      "Epoch: 25/25... Step: 3340... Loss: 1.1433... Val Loss: 1.3835\n",
      "Epoch: 25/25... Step: 3350... Loss: 1.1437... Val Loss: 1.3893\n",
      "Epoch: 25/25... Step: 3360... Loss: 1.1374... Val Loss: 1.3811\n",
      "Epoch: 25/25... Step: 3370... Loss: 1.1309... Val Loss: 1.3861\n",
      "Epoch: 25/25... Step: 3380... Loss: 1.1247... Val Loss: 1.3781\n",
      "Epoch: 25/25... Step: 3390... Loss: 1.1234... Val Loss: 1.3895\n",
      "Epoch: 25/25... Step: 3400... Loss: 1.1410... Val Loss: 1.3891\n",
      "Epoch: 25/25... Step: 3410... Loss: 1.1464... Val Loss: 1.3853\n",
      "Epoch: 25/25... Step: 3420... Loss: 1.1238... Val Loss: 1.3887\n",
      "Epoch: 25/25... Step: 3430... Loss: 1.1482... Val Loss: 1.3910\n",
      "Epoch: 25/25... Step: 3440... Loss: 1.1300... Val Loss: 1.3845\n",
      "Epoch: 25/25... Step: 3450... Loss: 1.1283... Val Loss: 1.3834\n",
      "Epoch: 25/25... Step: 3460... Loss: 1.1466... Val Loss: 1.3859\n",
      "Epoch: 25/25... Step: 3470... Loss: 1.1456... Val Loss: 1.3785\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the best model\n",
    "\n",
    "To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we'll save the model so we can load it again later if we need too. Here I'm saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Now that the model is trained, we'll want to sample from it. To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorcial probability distribution over all the possible characters. We can make the sampled text more reasonable but less variable by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, and she seemed to\n",
      "have a tears of himself, but as though he did not tell them his face. He\n",
      "was not settled at him, and with whom he had already sat down, and that\n",
      "it was impossible to say.\"\n",
      "\n",
      "\"That's not somewhere the point to her.\" They despring them at that\n",
      "mistake and him. The marshes was the chincer, was in a glance of\n",
      "assisted at the corricted service. But when she had so given on the\n",
      "presence.\n",
      "\n",
      "\"I have been always to the than except into a bare, but there,\n",
      "as it is to be supposed.\" Again he they had not all to be\n",
      "about the mistares and thousands as so towards it, and what the\n",
      "mather was to the carriage with silence, and walked a love with which\n",
      "he had true, that he had an acquaintance. The creeming\n",
      "sense of the colorel sorts, and at the comminterness of his own\n",
      "her hands, and had belonged to the peasants, he had a country peasant\n",
      "his breathed handed, he sat sight of the same time as his friends, to see\n",
      "where the household has taken in out of the same time and his sister\n",
      "in a long wife.\n",
      "\n",
      "\"I'm not going, and you must be definite that, and I've taken up in the matter,\n",
      "that the perplehey was to go and should be doing. This was an\n",
      "official serious towards the same ways, and shaking it the child.\n",
      "\n",
      "\"I should have sent on a prince,\" the point said, at some time, he would\n",
      "be thinking of that there was so a good first and happy before her former\n",
      "and staying at his breaking and, and the divorce, there was nothing and\n",
      "that struck any other self-conventional and annight for him\n",
      "and the misingry, and alone was a good hair, and talking away with\n",
      "her. And telling him to an except only a lighted attention of sorried\n",
      "soffed and her, and had so callly the province was sitting in the\n",
      "children at the trumples of the matter, struck him to see that he were\n",
      "to go out of the path that shoted a look of half-pate, was still all to\n",
      "holital. Sergey Ivanovitch was the same tell him with the contrary\n",
      "to the crowd of his coat and wondering that it had\n",
      "all suddened and so well of him.\n",
      "\n",
      "The so\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said to her.\n",
      "\n",
      "The simplest shame was not happy, and that the marshal, with her\n",
      "strange side would as a chance, and with his head horses, and the point of\n",
      "timp she fancied that something was stuck by the secrets were as he\n",
      "wanted, the common. They were continually\n",
      "to grief the clever, that with service of the second, this was his\n",
      "wife.\n",
      "\n",
      "\"You say it's supposing it, I'm all simpler, though I can't see you thank God\n",
      "in that clear woman went to the same to the sense of hands of\n",
      "a significance after things about her the servant where a shinest must\n",
      "doing the children that suddenly came to be a shall on the soul it will\n",
      "change yesterday!\" he asked, stranging at the post, but she said to him, and\n",
      "he stupid at him, and would have been deserved in all this time as her stop,\n",
      "throbbing the propinting hers in his eyes, as his white stock on the\n",
      "wind of the sound of the drawing room, he walked to the cases, he fell that\n",
      "she would be decided, and the prince was\n",
      "complicated them all at once, he came in the world. He stood as\n",
      "answer that she did not know to him; he was supposing\n",
      "an else in haste to him, his wife showed her as after\n",
      "social sturies, the most only\n",
      "of the same and her lips seemed to her. And the crimson went up\n",
      "for the conviction of the same trive to tell him of all them.\n",
      "\n",
      "\"Why is you take his horse in him.... I've been many poor of socious\n",
      "professs of him. If I could not speak by the patures above it?\" he said\n",
      "at the party of the person on the colore,\n",
      "tried to dinner the captar, a precious stand, shateful the\n",
      "forest had to be so anything as he was surprised, and the sound was\n",
      "towards into his fect in the compart. The carriage carried often all\n",
      "today they should be suddred and saying all the chief condection of\n",
      "it. A store of a peculiar fell work as the sick man would have been in\n",
      "condition to see in the side of home the low constarations that he\n",
      "should have asked it, but he was not to be so in society of the classes\n",
      "with such a stolen of surplance with an answers of think a\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, cuda=True, top_k=5, prime=\"And Levin said\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
